{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to NLP Course\n",
    "\n",
    "## Content:\n",
    "1. Sentence Tokenization - Word Tokenization\n",
    "2. Text Cleaning\n",
    "3. Text Normalization\n",
    "4. Stemming\n",
    "5. Lemmaization\n",
    "6. POSTagging\n",
    "7. Named Entity Recognition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Sentence Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"\"\"Susmayın!\n",
    "             Saat 16'da Kadıköy'de buluşalım.\n",
    "            Şiddet canilik göstergesinin en üst sınırıdır.\n",
    "            Ailede şiddet geleceği karartır.\n",
    "            Kadına şisddet, insanlığa ihanettir.\n",
    "            Her sessiz kalınan şiddet bir gün sizi bulur.\n",
    "            Sevgi insanlığın, şiddet hayvanlığın kanunudur.\"\"\"\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Word Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentences = ['Susmayın!',\n",
    " \"Saat 16'da Kadıköy'de buluşalım.\",\n",
    " 'Şiddet canilik göstergesinin en üst sınırıdır.',\n",
    " 'Ailede şiddet geleceği karartır.',\n",
    " 'Kadına şisddet, insanlığa ihanettir.',\n",
    " 'Her sessiz kalınan şiddet bir gün sizi bulur.',\n",
    " 'Sevgi insanlığın, şiddet hayvanlığın kanunudur.']\n",
    "\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.append(word_tokenize(sentence))\n",
    "\n",
    "words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with Zemberek"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from zemberek import (\n",
    " TurkishSpellChecker,\n",
    " TurkishSentenceNormalizer,\n",
    " TurkishSentenceExtractor,\n",
    " TurkishMorphology,\n",
    " TurkishTokenizer\n",
    ")\n",
    "\n",
    "tokenizer = TurkishTokenizer.DEFAULT\n",
    "sentence = \"İstanbul'a merhaba!\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "for token in tokens:\n",
    " print('Content = ', token.content)\n",
    " print('Type = ', token.type_.name)\n",
    " print('Start = ', token.start)\n",
    " print('Stop = ', token.end, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Text Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Removing punctuation and special letters.\n",
    "import string\n",
    "message = 'Dil işlemede kullanılan kütüphaneler: nltk, spacy, scikit-learn vb.'\n",
    "print(message.translate(str.maketrans('', '', string.punctuation)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acaba', 've', 'için'}\n"
     ]
    }
   ],
   "source": [
    "# Cleaning ineffective words\n",
    "stop_words = ['acaba', 've', 'bir', 'birçok', 'ama', 'için']\n",
    "message = 'Acaba metindeki dolgu kelimelerini bulmak ve temizlemek için ne yapılmalı?'\n",
    "S1 = set(stop_words)\n",
    "S2 = set(message.lower().split())\n",
    "print(S1.intersection(S2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Text Normalization\n",
    "Text should have the same letter cases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "message = 'Metin içinde geçen kelime sayısı o metin için bir özniteliktir.'\n",
    "message.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Text,non-canonical words should converse.\n",
    "Spellchecks should normalized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Stemming\n",
    "### NLTK Porter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "word = 'civilizations'\n",
    "stemmed_word = porter_stemmer.stem(word)\n",
    "\n",
    "stemmed_word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK Snowball (2nd Porter Stemmer) for ENG\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sb_stemmer = SnowballStemmer(language='english')\n",
    "word = 'civilizations'\n",
    "stemmed_word = sb_stemmer.stem(word)\n",
    "\n",
    "stemmed_word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLTK Snowball (2nd Porter Stemmer) for TR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "tr_stemmer = TurkishStemmer()\n",
    "stemmed_word = tr_stemmer.stemWord('çiçekler')\n",
    "stemmed_word\n",
    "\n",
    "# In fact, it is mostly working for lemmatization\n",
    "lemmatized_word = tr_stemmer.stemWord('çiçeklerler')\n",
    "lemmatized_word\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Lemmatization\n",
    "### spaCy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "word = nlp('civilizations')\n",
    "\n",
    "print (\" \".join([token.lemma_ for token in word]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with Zemberek"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from zemberek import (\n",
    " TurkishSpellChecker,\n",
    " TurkishSentenceNormalizer,\n",
    " TurkishSentenceExtractor,\n",
    " TurkishMorphology,\n",
    " TurkishTokenizer\n",
    ")\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "sentence_1 = \"Ayının kürkü kahverengidir.\"\n",
    "morph_analysis_1 = morphology.analyze(sentence_1)\n",
    "\n",
    "sentence_2 = \"Kitabımızı aldık.\"\n",
    "morph_analysis_2 = morphology.analyze(sentence_2)\n",
    "\n",
    "morph_analysis_1\n",
    "morph_analysis_2.EMPTY_INPUT_RESULT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. POSTagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with NLTK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokens = nltk.word_tokenize(\"Can you buy me a red chili pepper from grocery?\")\n",
    "print(\"Part of Speech: \", nltk.pos_tag(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with spaCy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"I want an early upgrade\")\n",
    "for token in doc:\n",
    " print(token.text, token.pos_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Named Entity Recognition\n",
    "\n",
    "#### with NLTK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "from nltk import ne_chunk\n",
    "sentence = \"Legendary scientist Albert Einstein is born in Ulm, Germany.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged_tokens)\n",
    "entities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### with spaCY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = nlp(\"Michael Jordan is a professor at Berkeley\")\n",
    "entities = sentence.ents\n",
    "\n",
    "print([(entity.text, entity.label_)for entity in entities])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}